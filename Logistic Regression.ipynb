{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### binomial logistic regression model\n",
    "$P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)}$\n",
    "\n",
    "$P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$\n",
    "\n",
    "$logit(p)=log\\frac{p}{1-p}=w\\cdot x$\n",
    "\n",
    "### multi-nomial logistic regression model\n",
    "$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)}, k=1,2,\\cdots,K$\n",
    "\n",
    "$P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)}$\n",
    "\n",
    "## maximum entropy model\n",
    "\n",
    "&emsp; 给定一个训练数据集，可以确定联合分布P(x, y)的经验分布和边缘分布P(X)的经验分布，分别以$\\tilde{P}(X, Y)$和$\\tilde{P}(X)$表示\n",
    "\n",
    "$\\tilde{P}(X=x, Y=y)=\\frac{\\upsilon(X=x, Y=y)}{N}$\n",
    "\n",
    "$\\tilde{P}(X=x)=\\frac{\\upsilon(X=x)}{N}$\n",
    "\n",
    "&emsp; 特征函数$f(x, y)$描述输入x和输出y之间的某一个事实，其定义为\n",
    "\n",
    "$$f(x, y)=\\begin{cases}\n",
    "    1, & x与y满足某一事实\\\\\n",
    "    0, & otherwise\n",
    "    \\end{cases}$$\n",
    "\n",
    "&emsp; 特征函数$f(x, y)$关于经验分布$\\tilde{P}(X, Y)$的期望值，用$E_{\\tilde{P}}(f)$表示\n",
    "\n",
    "$E_{\\tilde{P}}(f)=\\sum_{x, y}\\tilde{P}(X, Y)f(x, y)$\n",
    "\n",
    "&emsp; 特征函数$f(x, y)$关于模型$P(Y|X)$经验分布$\\tilde{P}(X, Y)$的期望值，用$E_{P}(f)$表示\n",
    "\n",
    "$E_{P}(f)=\\sum_{x, y}\\tilde{P}(X)P(Y|X)f(x, y)$\n",
    "\n",
    "&emsp; 若模型能够获取训练数据中的信息，那么可以假设这两个期望值相等，将其作为模型学习的约束条件，最大熵模型的学习等价于约束最优化问题：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "min_{P\\in C} \\ &\\sum_{x, y}\\tilde{P}(X)P(Y|X)logP(Y|X)\\\\\n",
    "s.t. \\ &E_{P}(f_i)=E_{\\tilde{P}}(f_i)\\\\\n",
    "      &\\sum_yP(y|x)=1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## 牛顿法与拟牛顿法\n",
    "\n",
    "$x^{(k+1)}=x^{(k)}-H_{k}^{-1}\\nabla f(x^{(k)})$\n",
    "\n",
    "&emsp; 在拟牛顿法中将$G_k$作为$H_k^{-1}$的近似\n",
    "\n",
    "$G_{k+1}(\\nabla f(x^{(k+1)})-\\nabla f(x^{(k)}))=x^{(k+1)}-x^{(k)}$\n",
    "\n",
    "### BFGS\n",
    "\n",
    "* $g$为$f$的梯度，$B$为$f$的海森矩阵近似，选定初始点$w^{(0)}$，取$B_0$为正定对称矩阵\n",
    "\n",
    "* $g_k=g(w^{(k)})$，若$g_k=0$，停止计算\n",
    "\n",
    "* 由$B_kp_k=-g_k$求出$p_k$,搜索$\\lambda_k=min_{\\lambda \\geq 0}f(w^{(k)}+\\lambda p_k)\n",
    "\n",
    "* $w^{(k+1)}=w^{(k)}+\\lambda_k p_k$\n",
    "\n",
    "* $B_{k+1}=B_k+\\frac{y_ky_k^T}{y_k^T\\delta_k}-\\frac{B_k\\delta_k\\delta_k^TB_k}{\\delta_k^TB_k\\delta_k},\\ y_k=g_{k+1}-g_k,\\ \\delta_k=w^{(k+1)}-w^{(k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (100,)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = X[:100], y[:100]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR():\n",
    "    def __init__(self, lr=0.01, max_iter=100):\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.weight = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "        self.weight = np.zeros((D, ))\n",
    "        for i in range(self.max_iter):\n",
    "            y_hat = self.predict(X)\n",
    "            dw = np.matmul(X.T, y_hat - y)\n",
    "            self.weight -= self.lr * dw\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weight is None:\n",
    "            print(\"no training\")\n",
    "            return\n",
    "        \n",
    "        s = np.exp(np.matmul(X, self.weight)).reshape((-1, 1))\n",
    "        s = np.concatenate((np.ones((X.shape[0], 1)), s), axis=1)\n",
    "        return np.argmax(s, axis=1)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return np.sum(y_hat == y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\msi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "indices = np.array(range(100))\n",
    "np.random.shuffle(indices)\n",
    "X_train, y_train = X[indices[:90]], y[indices[:90]]\n",
    "X_test, y_test = X[indices[90:]], y[indices[90:]]\n",
    "\n",
    "logit = LogisticRegression()\n",
    "model = LR()\n",
    "model.fit(X_train, y_train)\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test), logit.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
