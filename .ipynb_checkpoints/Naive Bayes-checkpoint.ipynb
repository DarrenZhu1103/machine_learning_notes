{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes简介\n",
    "\n",
    "&emsp; 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。\n",
    "\n",
    "## 基本方法\n",
    "\n",
    "&emsp; 朴素贝叶斯法通过训练数据集学习联合分布$P(X, Y)$。通过学习先验概率分布以及条件概率分布得出。\n",
    "\n",
    "&emsp; 先验概率分布：\n",
    "\n",
    "$P(y=c_k),k=1,2, \\cdots,K$\n",
    "\n",
    "&emsp; 条件概率分布：\n",
    "\n",
    "$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots, X^{(n)}=x^{(n)}|y=c_k), k=1,2,\\cdots,K$\n",
    "\n",
    "&emsp; 朴素贝叶斯法对条件概率分布作了条件独立性的假设，这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率：\n",
    "\n",
    "$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots, X^{(n)}=x^{(n)}|y=c_k)=\\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$\n",
    "\n",
    "&emsp; 使用朴素贝叶斯法分类时，对给定的输入x，通过学习到的模型计算后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为x的类输出。于是朴素贝叶斯分类器可表示为：\n",
    "\n",
    "$y=f(x)=arg\\,\\max_{c_k}\\frac{P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_iP(Y=c_i)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_i)}$\n",
    "\n",
    "&emsp; 去掉相同的分母进行简化：\n",
    "\n",
    "$y=f(x)=arg\\,\\max_{c_k}P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)$\n",
    "\n",
    "## 极大似然参数估计\n",
    "\n",
    "&emsp; 首先计算先验概率及条件概率：\n",
    "\n",
    "$P(y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2, \\cdots,K$\n",
    "\n",
    "\n",
    "$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},\\,y_i=c_k)}{\\sum_{i=1}^{N}I(y_i=c_k)}$\n",
    "\n",
    "&emsp; 确定实例x的分类：\n",
    "\n",
    "$y=arg\\,\\max_{c_k}P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)$\n",
    "\n",
    "## 贝叶斯估计\n",
    "\n",
    "&emsp; 上述极大似然估计可能会产生所要估计的概率值为0的情况，这会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。\n",
    "\n",
    "&emsp; 先验概率及条件概率的贝叶斯估计：\n",
    "\n",
    "$P(y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)+\\lambda}{N+k\\lambda},k=1,2, \\cdots,K$\n",
    "\n",
    "\n",
    "$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl},\\,y_i=c_k)+\\lambda}{\\sum_{i=1}^{N}I(y_i=c_k)+S_j\\lambda},l=1,2, \\cdots,S_j$\n",
    "\n",
    "&emsp; 常取$\\lambda=1$，这时称为拉普拉斯平滑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def __init__(self, lambd):\n",
    "        self.prior_prob = None\n",
    "        self.cond_prob = []\n",
    "        self.lambd = lambd\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "        stat_y = np.bincount(y)\n",
    "        self.num_of_class = stat_y.shape[0]\n",
    "        self.prior_prob = (stat_y + self.lambd) / (N + self.num_of_class * self.lambd)\n",
    "        \n",
    "        for i in range(D):\n",
    "            x = X[:, i]\n",
    "            x_value = np.unique(x)\n",
    "            s = x_value.shape[0]\n",
    "            prob = np.zeros((s, self.num_of_class))\n",
    "            self.cond_prob.append(prob)\n",
    "            for j in range(s):\n",
    "                y_set = y[x==x_value[j]]\n",
    "                stat_xy = np.zeros((self.num_of_class, ))\n",
    "                for k in range(self.num_of_class):\n",
    "                    stat_xy[k] = np.sum(y_set == k)\n",
    "                prob[j] = (stat_xy + self.lambd) / (stat_y + s * self.lambd)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.prior_prob is None:\n",
    "            print(\"no training\")\n",
    "            return\n",
    "        \n",
    "        N, D = X.shape\n",
    "        prob = np.ones((N, self.num_of_class))\n",
    "        for i in range(D):\n",
    "            x = X[:, i]\n",
    "            prob *= self.cond_prob[i][x]\n",
    "        prob *= self.prior_prob\n",
    "        return np.argmax(prob, axis=1)\n",
    "            \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return np.sum(y_hat == y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88 0.91\n"
     ]
    }
   ],
   "source": [
    "X = X.astype(int)\n",
    "X_train, y_train = X[:1697], y[:1697]\n",
    "X_test, y_test = X[1697:], y[1697:]\n",
    "\n",
    "clf = MultinomialNB()\n",
    "model = NaiveBayes(1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test), model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
