{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维与度量学习\n",
    "\n",
    "## 低维嵌入\n",
    "\n",
    "&emsp; 在高维情形下，出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为维数灾难。缓解维数灾难的一个重要途径是降维，亦称维数约减，即通过某种数学变换将原始高维属性空间转变为一个低维子空间，在这个子空间中样本密度大幅提高，距离计算也变得更为容易。\n",
    "\n",
    "&emsp; 多维缩放(MDS)是一种使得原始空间中样本之间的距离在低维空间中得以保持的降维方法。假定m个样本在原始空间的距离矩阵为$D\\in R^{m\\times m}$，其第$i$行$j$列的元素$dist^{ij}$为样本$x_i$到$x_j$的距离。MDS的目标是获得样本在$d'$维空间的表示$Z\\in R^{d'\\times m}, d'\\leq d$，且任意两个样本在$d'$维空间中的欧式距离等于原始空间中的距离，即$||z_i-z_j||=dist_{ij}$\n",
    "\n",
    "&emsp; 令$B=Z^TZ\\in R^{m\\times m}$，其中$B$为降维后样本的内积矩阵，$b_{ij}=z_i^Tz_j$，有\n",
    "\n",
    "$$dist_{ij}^2=b_{ii}+b{jj}-2b_{ij}$$\n",
    "\n",
    "&emsp; 为便于讨论，令降维后样本$Z$被中心化，即$\\sum_{i=1}^mb_{ij}=\\sum_{j=1}^mb_{ij}=0$\n",
    "\n",
    "$$\\sum_{i=1}^mdist_{ij}^2=tr(B)+mb_{jj}$$\n",
    "\n",
    "$$\\sum_{j=1}^mdist_{ij}^2=tr(B)+mb_{ii}$$\n",
    "\n",
    "$$\\sum_{i=1}^m\\sum_{j=1}^mdist_{ij}^2=2mtr(B)$$\n",
    "\n",
    "&emsp; $tr(B)=\\sum_{i=1}^m||z_i||^2$，令\n",
    "\n",
    "$$dist_{i\\cdot}=\\frac{1}{m}\\sum_{j=1}^mdist_{ij}^2$$\n",
    "\n",
    "$$dist_{\\cdot j}=\\frac{1}{m}\\sum_{i=1}^mdist_{ij}^2$$\n",
    "\n",
    "$$dist_{\\cdot\\cdot}=\\frac{1}{m^2}\\sum_{i=1}^m\\sum_{j=1}^mdist_{ij}^2$$\n",
    "\n",
    "&emsp; 可推出\n",
    "\n",
    "$$b_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{i\\cdot}-dist_{\\cdot j}+dist_{\\cdot\\cdot})$$\n",
    "\n",
    "&emsp; 对矩阵$B$进行特征值分解，$B=V\\Lambda V^T$，$V$为特征向量矩阵，$\\Lambda=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d),\\ \\lambda_1\\geq \\lambda_2\\geq \\cdots\\geq \\lambda_d$，令$\\tilde{\\Lambda}=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_d'), \\tilde{V}$表示相应的特征向量矩阵，则$Z$可表示为\n",
    "\n",
    "$$Z=\\tilde{\\Lambda}^{1/2}\\tilde{V}^T$$\n",
    "\n",
    "## 主成分分析\n",
    "\n",
    "&emsp; 主成分分析(Princeple Component Analysis,PCA)是最常用的一种降维方法：\n",
    "\n",
    "* 对所有样本进行中心化$x_i=x_i-\\frac{1}{m}\\sum_{i=1}^mx_i$\n",
    "\n",
    "* 计算样本的协方差矩阵$XX^T$\n",
    "\n",
    "* 对协方差矩阵进行特征值分解\n",
    "\n",
    "* 取最大的$d'$个特征值所对应的特征向量$w_1,w_2,\\cdots,w_{d'}$\n",
    "\n",
    "## 核化线性降维\n",
    "\n",
    "&emsp; 线性降维方法假设从高维空间到低维空间的的函数映射是线性的，然而，在一些任务中，可能需要非线性映射才能找到恰当的低维嵌入。非线性降维的一种常用的方法，是基于核技巧对线性降维方法进行“核化”，核主成分分析(KPCA)为其中之一。\n",
    "\n",
    "&emsp; 假定我们在高维特征空间中把数据投影到由$W=(w_1,w_2.\\cdots,w_d)$确定的超平面上，则有\n",
    "\n",
    "$$(\\sum_{i=1}^m\\Phi(x_i)\\Phi(z_i)^T)w_j=\\lambda_jw_j,\\ w_j=\\sum_{i=1}^m\\Phi(x_i)\\frac{\\Phi(x_i)^Tw_j}{\\lambda_j}=\\sum_{i=1}^m\\Phi(x_i)\\alpha_i^j$$\n",
    "\n",
    "&emsp; 引入核函数\n",
    "\n",
    "$$K(x_i, x_j)=Phi(x_i)^T\\Phi(x_j)$$\n",
    "\n",
    "&emsp; 将上面的式子带入\n",
    "\n",
    "$$(\\sum_{i=1}^m\\Phi(x_i)\\Phi(z_i)^T)w_j=\\lambda_jw_j$$\n",
    "\n",
    "&emsp; 得\n",
    "\n",
    "$$K\\alpha^j=\\lambda_j\\alpha^j$$\n",
    "\n",
    "&emsp; 其中，$(K)_{ij}=K(x_i, x_j),\\ \\alpha^j=(\\alpha_1^j;\\alpha_2^j;cdots;\\alpha_m^j)^T$，取最大的$d'$个特征值即可\n",
    "\n",
    "&emsp; 对新样本$x$，投影后第$j$个坐标为\n",
    "\n",
    "$$z_j=w_j^T\\Phi(x)=\\sum_{i=1}^m\\alpha_i^j\\Phi(x_i)\\Phi(x)=\\sum_{i=1}^m\\alpha_i^jK(x_i, x)$$\n",
    "\n",
    "## 流形学习\n",
    "\n",
    "&emsp; 流形学习(manifold learning)是一类借鉴了拓扑流形概念的降维方法。\n",
    "\n",
    "### 等度量映射\n",
    "\n",
    "&emsp; 等度量映射(Isomap)的基本出发点，是认为低维流形嵌入到高维空间之后，直接在高维空间中计算直线距离具有误导性，因为高维空间中的直线距离在低维嵌入流形上是不可达的。此时可利用流形在局部上与欧式空间同胚这个性质，对每个店基于欧式距离找出其临近点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接。\n",
    "\n",
    "* 对每个点确立近邻点，将近邻点之间的距离设置为欧式距离，非近邻点之间的距离为无限大\n",
    "\n",
    "* 调用最短路径算法计算任意两样本点之间的距离，再使用MDS\n",
    "\n",
    "### 局部线性嵌入\n",
    "\n",
    "&emsp; 与等度量映射试图保持近邻样本之间的距离不同，局部线性嵌入(LLE)试图保持邻域内样本间的线性关系。假定样本点$x_i$的坐标能通过它的邻域样本$x_j,x_k,x_l$的坐标通过线性组合重构出来，即\n",
    "\n",
    "$$x_i=w_{ij}x_j+w_{ik}x_k+w_{ih}x_l$$\n",
    "\n",
    "&emsp; LLE希望上式关系再低维空间中得以保持\n",
    "\n",
    "* 对所有的样本$x$确定近邻，计算$w_{ij}$，组成$W$(没有计算的部分置0)\n",
    "\n",
    "* 计算$M=(I-W)^T(I-W)$，返回$M$的最小$d'$个特征值对应的特征向量\n",
    "\n",
    "## 度量学习\n",
    "\n",
    "&emsp; 在机器学习中，对高维数据进行降维的主要目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应看在样本属性上定义的一个距离度量，而寻找合适的空间，实质上就是在寻找一个合适的距离度量，即度量学习的基本动机。\n",
    "\n",
    "&emsp; 对两个$d$维样本$x_i$和$x_j$，他们之间的平方欧式距离可写为\n",
    "\n",
    "$$dist_{ed}^2(x_i,x_j)=||x_i-x_j||_2^2=dist_{ij,1}^2+dist_{ij,2}^2+\\cdots +dist_{ij,d}^2$$\n",
    "\n",
    "&emsp; 若假定不同属性的重要性不同，则可引入属性权重$w$，得到\n",
    "\n",
    "$$dist_{wed}^2(x_i,x_j)=||x_i-x_j||_2^2=w_1\\cdot dist_{ij,1}^2+w_2\\cdot dist_{ij,2}^2+\\cdots +w_d\\cdot dist_{ij,d}^2=(x_i-x_j)^TW(x_i-x_j)$$\n",
    "\n",
    "&emsp; 其中$w_i\\geq 0,\\ W=diag(w)$是一个对角矩阵，$(W)_{ii}=w_i$，在这里$W$的非对角元素均为0，这意味着坐标轴是正交的，即属性之间无关；但现实问题中往往会出现相关的属性，为此，将$W$替换为一个普通的半正定对称矩阵$M$，于是得到了马氏距离\n",
    "\n",
    "$$dist_{med}^2(x_i,x_j)==(x_i-x_j)^TM(x_i-x_j)=||x_i-x_j||_M^2$$\n",
    "\n",
    "&emsp; 度量学习即对$M$进行学习"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
