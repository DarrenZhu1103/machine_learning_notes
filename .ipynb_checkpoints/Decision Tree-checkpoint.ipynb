{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树模型\n",
    "\n",
    "&emsp; 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。\n",
    "\n",
    "&emsp; 用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值，如此递归地对实例进行测试并分配，直到达到叶结点，最后将实例分到叶结点的类中。\n",
    "\n",
    "&emsp; 决策树还表示给定特征条件下类的条件概率分布，这一条件概率分布定义在特征空间的一个划分上，将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。\n",
    "\n",
    "&emsp; 决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使其能够对实例进行正确的分类。决策树的学习本质上使从训练数据集中归纳出一组分类规则，构建一个与训练数据矛盾较小的决策树，同时具备很好的泛化能力。\n",
    "\n",
    "&emsp; 决策树学习的策略使将损失函数最小化。学习问题是从所有的可能的决策树选择出最优决策树，这是一个NP-complete问题，在现实中决策树学习算法通常采取启发式算法，近似求解。\n",
    "\n",
    "&emsp; 决策树学习常用的算法有ID3、C4.5与CART。\n",
    "\n",
    "## 信息增益\n",
    "\n",
    "&emsp; 熵表示随机变量不确定性的度量，设X是一个取有限个值的离散随机变量，则随机变量X的熵定义为：\n",
    "\n",
    "$H(X)=-\\sum_{i=1}^{n}p_ilogp_i$\n",
    "\n",
    "&emsp; 熵越大，随机变量的不确定性就越大。\n",
    "\n",
    "&emsp; 条件熵$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期望：\n",
    "\n",
    "$H(Y|X)=\\sum_{i=1}^{n}p_iH(Y|X=x_i)$\n",
    "\n",
    "&emsp; 这里，$p_i=P(X=x_i), i=1,2, \\cdots, n$\n",
    "\n",
    "&emsp; 信息增益表示特征X的信息而使得类Y的信息不确定性减少的程度。特征A对训练数据集D的信息增益$g(D, A)$，定义为：\n",
    "\n",
    "$g(D, A)=H(D)-H(D|A)$\n",
    "\n",
    "## 计算信息增益\n",
    "\n",
    "&emsp; 设训练数据集为D，|D|表示其样本容量，设有K个类$C_k, k=1,2, \\cdots, K$，$|C_k|$为属于类$C_k$的样本个数，设特征A有n个不同的取值，根据特征A把D划分为n个子集，$|D_i|$为$D_i$的样本个数，记子集$D_i$中属于$C_k$的样本集合为$D_{ik}$，计算信息增益：\n",
    "\n",
    "$H(D)=-\\sum_{k=1}^{K}\\frac{|C_k|}{|D|}log\\frac{|C_k|}{|D|}$\n",
    "\n",
    "$H(D|A)=-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i)=\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}\\sum_{k=1}^{K}\\frac{|D_{ik}|}{|D_i|}log\\frac{|D_{ik}|}{|D_i|}$\n",
    "\n",
    "$g(D, A)=H(D)-H(D|A)$\n",
    "\n",
    "## 信息增益比\n",
    "\n",
    "&emsp; 特征A对训练数据集D的信息增益比$g_R(D, A)$定义为其信息增益与训练数据集D关于特征A的熵$H_A(D)$之比，即：\n",
    "\n",
    "$g_R(D, A)=\\frac{g(D, A)}{H_A(D)}$\n",
    "\n",
    "其中，$H_A(D)=-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}log\\frac{|D_i|}{|D|}$。\n",
    "\n",
    "## ID3算法\n",
    "\n",
    "1. 若D中所有实例属于同一类$C_k$，则T为单节点树，并将类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "2. 若$A=\\emptyset$，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "3. 否则，计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$\n",
    "\n",
    "4. 若$A_g$的信息增益小于阈值$\\varepsilon$，则置T为单节点树，并将D中的实例数最大的类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "5. 否则，对$A_g$的每一可能值$a_i$，根据$A_g=a_i$将D分割为若干个非空子集$D_i$，将$D_i$中实例最大的类进行标记，构建子结点，由结点及其子结点构成树T，返回T\n",
    "\n",
    "6. 对第i个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用前五步，得到子树$T_i$\n",
    "\n",
    "## C4.5生成算法\n",
    "\n",
    "&emsp; 与ID3算法大致相同，在生成过程中用信息增益比来选择特征。\n",
    "\n",
    "## 决策树的剪枝\n",
    "\n",
    "&emsp; 决策树生成算法产生的树往往对训练数据的分类很准确，但会出现过拟合现象，解决这个问题的方法是考虑决策树的复杂度，对已生成的决策树进行简化，该过程被称为剪枝。\n",
    "\n",
    "&emsp; 剪枝往往通过极小化决策树整体的损失函数来实现，设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$H_t(T)$为叶结点t上的经验熵，$\\alpha >= 0$为参数，则决策树学习的损失函数定义为：\n",
    "\n",
    "$C_{\\alpha}(T)=-\\sum_{i=1}^{|T|}N_tH_t(T)+\\alpha|T|=-\\sum_{i=1}^{|T|}\\sum_{k}N_{tk}log\\frac{N_{tk}}{N_t}+\\alpha|T|$\n",
    "\n",
    "&emsp; 剪枝算法：\n",
    "\n",
    "1. 计算每个结点的经验熵\n",
    "\n",
    "2. 递归地从树的叶结点向上回缩，设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，若$C_{\\alpha}(T_A)<=C_{\\alpha}(T_B)$，则父结点变为新的叶结点。重复直到不能继续。\n",
    "\n",
    "## CART算法\n",
    "\n",
    "&emsp; 分类树与回归树(classification and regression tree, CART)模型为在给定的输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法，其假设决策树是二叉树，内部节点特征的取值为“是”和“否”。\n",
    "\n",
    "### CART生成\n",
    "#### 回归树的生成\n",
    "&emsp; 一个回归树模型对应输入空间的一个划分以及在划分的单元上的输出值，可表示为\n",
    "\n",
    "$f(x)=\\sum_{m=1}^{M}c_mI(x\\in R_m)$\n",
    "\n",
    "&emsp; 当输入空间划分确定时，可以用平方误差$\\sum_{x_i\\in R_m}(y_i-f(x_i))^2$表示训练数据的预测误差，$c_m$的最优值即$\\hat{c_m}=ave(y_i|x_i\\in R_m)$\n",
    "\n",
    "&emsp; 最小二乘回归树生成算法\n",
    "\n",
    "* 选择最优切分变量j与切分点s，求解\n",
    "\n",
    "$min_{j,s}[min_{c1}\\sum_{x_i\\in R_1(j,s)}{(y_i-c_1)^2}+min_{c2}\\sum_{x_i\\in R_2(j,s)}{(y_i-c_2)^2}]$\n",
    "\n",
    "&emsp; 遍历变量j，对固定的切分变量j扫描切分点s，解上述最小化问题\n",
    "\n",
    "* 用选定的对(j, s)划分区域并决定相应的输出值：\n",
    "\n",
    "$R_1(j,s)={x|x^(j)<=s}, \\, R_2(j,s)={x|x^(j)>s}$\n",
    "\n",
    "$\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, \\, m=1,2$\n",
    "\n",
    "* 生成决策树：\n",
    "\n",
    "$f(x)=\\sum_{m=1}^{M}c_mI(x\\in R_m)$\n",
    "\n",
    "#### 分类树的生成\n",
    "&emsp; 分类问题中，假设有K个类，则概率分布的基尼系数代表集合D的不确定性，定义为\n",
    "\n",
    "$Gini(p)=1-\\sum_{k=1}^Kp_k^2, Gini(D)=1-\\sum_{k=1}^K\\frac{|C_k}{|D|}^2$\n",
    "\n",
    "&emsp; 若样本集合D根据特征A是否取某一科能值a被分割成两个部分，则在特征A的条件下，集合D的基尼指数定义为\n",
    "\n",
    "$Gini(D, A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$\n",
    "\n",
    "&emsp; 分类树生成算法\n",
    "\n",
    "1. 设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，此时，对每一个特征A，对其所有可能取的每个值a，根据样本点对A=a的测试为“是”或“否”将D分为两个部分\n",
    "\n",
    "2. 在所有可能的特征A以及他们所有可能的切分点a中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点，从现结点生成两个子结点，将训练数据分配到两个子结点中，重复这两步直至满足停止条件\n",
    "\n",
    "### CART剪枝\n",
    "\n",
    "1. 设$k=0, T=T_0, \\alpha=+\\infty$\n",
    "\n",
    "2. 自下而上地对各内部结点t计算$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1},\\,\\alpha=min(\\alpha,\\,g(t))$\n",
    "\n",
    "3. 对$g(t)=\\alpha$的内部结点t进行剪枝，并对叶结点t以多数表决法决定其类，得到树T\n",
    "\n",
    "4. 设$k=k+1,\\,\\alpha_k=\\alpha,\\,T_k=T$，若$T_k$不是由根节点及两个叶结点构成的树，则返回步骤2\n",
    "\n",
    "5. 采用较差验证法在子树序列$T_0,\\,T_1,\\,T_2,\\cdots,T_n$中选出最优子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, value, H_t, children=None, index=None, is_leaf=False):\n",
    "        self.H_t = H_t\n",
    "        self.children = children\n",
    "        self.index = index\n",
    "        self.value = value\n",
    "        self.is_leaf = is_leaf\n",
    "\n",
    "class DT():\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y, method='ID3'):\n",
    "        '''\n",
    "        Input:\n",
    "        - X: of shape (N, D)\n",
    "        - y: of shape(N, )\n",
    "        '''\n",
    "        A = list(range(X.shape[1]))\n",
    "        self.tree = self._tree_construct(X, y, A, method)\n",
    "        \n",
    "    def _tree_construct(self, X, y, A, method='ID3'):\n",
    "        if np.unique(y).shape[0]==1:\n",
    "            leaf = Node(y[0], 0, is_leaf=True)\n",
    "            return leaf\n",
    "        \n",
    "        H_t = y.shape[0] * self._entropy(y)\n",
    "        value = np.argmax(np.bincount(y))\n",
    "        if not A:\n",
    "            leaf = Node(value, H_t, is_leaf=True)\n",
    "            return leaf\n",
    "        \n",
    "        if method == 'ID3':\n",
    "            f = self.information_gain\n",
    "        elif method == 'C4.5':\n",
    "            f = self.information_gain_ratio\n",
    "        else:\n",
    "            print('\"wrong method')\n",
    "            return\n",
    "        \n",
    "        index = A[0]\n",
    "        max_gain = f(X, y, index)\n",
    "        for i in range(1, len(A)):\n",
    "            j = A[i]\n",
    "            gain = f(X, y, j)\n",
    "            if gain > max_gain:\n",
    "                max_gain = gain\n",
    "                index = j\n",
    "        \n",
    "        # recursion\n",
    "        A = A.copy()\n",
    "        A.remove(index)\n",
    "        keys = np.unique(X[:, index])\n",
    "        x = X[:, index]\n",
    "        children = dict()\n",
    "        node = Node(value, H_t, children, index)\n",
    "        for key in keys:\n",
    "            mask = x == key\n",
    "            children[key] = self._tree_construct(X[mask], y[mask], A, method)\n",
    "        \n",
    "        return node\n",
    "        \n",
    "    def information_gain(self, X, y, index):\n",
    "        H_D = self._entropy(y)\n",
    "        x = X[:, index]\n",
    "        N = x.shape[0]\n",
    "        keys = np.unique(x)\n",
    "        H_DA = 0\n",
    "        for key in keys:\n",
    "            mask = x == key\n",
    "            H_DA += self._entropy(y[mask]) * np.sum(mask) / N\n",
    "        return H_D - H_DA\n",
    "    \n",
    "    def information_gain_ratio(self, X, y, index):\n",
    "        inf_gain = self.information_gain(X, y, index)\n",
    "        x = X[:, index]\n",
    "        H_AD = self._entropy(x)\n",
    "        return inf_gain / H_AD\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        p = np.bincount(y) / y.shape[0]\n",
    "        p[p==0] = 1\n",
    "        H_D = -np.sum(p * np.log2(p))\n",
    "        return H_D\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.tree:\n",
    "            print(\"no training\")\n",
    "            return\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        y_hat = np.zeros((N, ))\n",
    "        for i in range(N):\n",
    "            x = X[i]\n",
    "            node = self.tree\n",
    "            while not node.is_leaf:\n",
    "                if x[node.index] in node.children:\n",
    "                    node = node.children[x[node.index]]\n",
    "                else:\n",
    "                    break\n",
    "            y_hat[i] = node.value\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return np.sum(y == y_hat) / y.shape[0]\n",
    "    \n",
    "    def prune(self, alpha=0):\n",
    "        if not self.tree:\n",
    "            return\n",
    "        self.cut(self.tree, alpha)\n",
    "    \n",
    "    def cut(self, node, alpha):\n",
    "        if node.is_leaf:\n",
    "            return 1, node.H_t\n",
    "        else:\n",
    "            H_t_sum = 0\n",
    "            n_sum = 0\n",
    "            for child in node.children.values():\n",
    "                n, H_t = self.cut(child, alpha)\n",
    "                H_t_sum += H_t\n",
    "                n_sum += n\n",
    "            if node.H_t - H_t_sum - alpha * (n_sum - 1) <= 0:\n",
    "                # prune\n",
    "                node.children = None\n",
    "                node.is_leaf = True\n",
    "                return 1, node.H_t\n",
    "            else:\n",
    "                return n_sum, H_t_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = np.array([['青年', '否', '否', '一般', '否'],\n",
    "               ['青年', '否', '否', '好', '否'],\n",
    "               ['青年', '是', '否', '好', '是'],\n",
    "               ['青年', '是', '是', '一般', '是'],\n",
    "               ['青年', '否', '否', '一般', '否'],\n",
    "               ['中年', '否', '否', '一般', '否'],\n",
    "               ['中年', '否', '否', '好', '否'],\n",
    "               ['中年', '是', '是', '好', '是'],\n",
    "               ['中年', '否', '是', '非常好', '是'],\n",
    "               ['中年', '否', '是', '非常好', '是'],\n",
    "               ['老年', '否', '是', '非常好', '是'],\n",
    "               ['老年', '否', '是', '好', '是'],\n",
    "               ['老年', '是', '否', '好', '是'],\n",
    "               ['老年', '是', '否', '非常好', '是'],\n",
    "               ['老年', '否', '否', '一般', '否'],\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = datasets[:, :4]\n",
    "label = datasets[:, 4]\n",
    "y = np.zeros((label.shape[0],), dtype=np.int64)\n",
    "y[label==\"是\"] = 1\n",
    "X = np.zeros(inf.shape, dtype=np.int64)\n",
    "X[inf=='是'] = 1\n",
    "X[inf=='中年'] = 1\n",
    "X[inf=='好'] = 1\n",
    "X[inf=='老年'] = 2\n",
    "X[inf=='非常好'] = 2\n",
    "model = DT()\n",
    "model.fit(X, y, 'C4.5')\n",
    "model.prune(0.5)\n",
    "title = [\"年龄\", \"有工作\", \"房子\", \"信贷\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 房子\n",
      "   否\n",
      "    有工作\n",
      "      否\n",
      "      是\n",
      "   是\n"
     ]
    }
   ],
   "source": [
    "def result_print(node, inf, title, space):\n",
    "    if node.is_leaf:\n",
    "        return\n",
    "    print(space, title[node.index])\n",
    "    for i, child in node.children.items():\n",
    "        print(space+\"  \", inf[node.index][int(i)])\n",
    "        result_print(child, inf, title, space+'   ')\n",
    "\n",
    "inform = [['青年', '中年', '老年'], ['否', '是'], ['否', '是'], ['一般', '好', '非常好']]\n",
    "result_print(model.tree, inform, title, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 0.5666666666666667 0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = X[:120], y[:120]\n",
    "X_test, y_test = X[120:], y[120:]\n",
    "clf = DecisionTreeClassifier()\n",
    "model_ID3 = DT()\n",
    "model_C4_5 = DT()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "model_ID3.fit(X_train, y_train)\n",
    "model_ID3.prune(0.5)\n",
    "model_C4_5.fit(X_train, y_train)\n",
    "model_C4_5.prune(0.5)\n",
    "\n",
    "print(clf.score(X_test, y_test), model_ID3.score(X_test, y_test), model_C4_5.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
