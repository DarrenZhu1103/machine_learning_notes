{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算学习理论\n",
    "\n",
    "## 基础知识\n",
    "\n",
    "&emsp; 计算学习理论是关于机器学习的理论基础，其目的维分析学习任务困难的本质，为学习算法提供理论保证，并根据分析结果指导算法设计，本章主要讨论二分类问题。\n",
    "\n",
    "&emsp; 给定样例集$D=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_m,y_m)\\}, x_i\\in X$，令$h$为$X$到$Y$的一个映射，其泛化误差为\n",
    "\n",
    "$$E(h;D)=P_{x\\sim D}(h(x)\\neq y)$$\n",
    "\n",
    "&emsp; $h$在$D$上的经验误差为\n",
    "\n",
    "$$\\tilde{E}(h;D)=\\frac{1}{m}\\sum_{i=1}^mI(h(x_i)\\neq y_i)$$\n",
    "\n",
    "&emsp; 令$\\epsilon$为$E(h)$的上限，通常用$\\epsilon$表示预先设定的学得模型所应满足的误差需求，亦称“误差参数”\n",
    "\n",
    "* Jensen不等式：对任意凸函数$f(x)$，有\n",
    "\n",
    "$$f(E(x))\\leq E(f(x))$$\n",
    "\n",
    "* Hoeffding不等式：若$x_1,x_2,\\cdots,x_m$为$m$个独立随机变量，且满足$0\\leq x_i\\leq 1$，则对任意$\\epsilon>0$，有\n",
    "\n",
    "$$P(\\frac{1}{m}\\sum_{i-1}^mx_i-\\frac{1}{m}\\sum_{i-1}^mE(x_i)\\geq\\epsilon)\\leq exp(-2m\\epsilon^2)$$\n",
    "\n",
    "$$P(|\\frac{1}{m}\\sum_{i-1}^mx_i-\\frac{1}{m}\\sum_{i-1}^mE(x_i)\\geq\\epsilon|)\\leq 2exp(-2m\\epsilon^2)$$\n",
    "\n",
    "* McDiarmid不等式：若$x_1,x_2,\\cdots,x_m$为$m$个独立随机变量，且对任意$1\\leq i\\leq m$，函数$f$满足$$sup_{x_1,x_2,\\cdots,x_m,x_i'}|f(x_1,x_2,\\cdots,x_m)-f(x_1,\\cdots,x_{i-1},x_i',x_{i+1}\\cdots,x_m)|\\leq c_i$$则对任意$\\epsilon>0$，有\n",
    "\n",
    "$$P(f(x_1,x_2,\\cdots,x_m)-E(f(x_1,x_2,\\cdots,x_m))\\geq\\epsilon)\\leq exp(\\frac{-2\\epsilon^2}{\\sum_ic_i^2})$$\n",
    "\n",
    "$$P(|f(x_1,x_2,\\cdots,x_m)-E(f(x_1,x_2,\\cdots,x_m))|\\geq\\epsilon)\\leq 2exp(\\frac{-2\\epsilon^2}{\\sum_ic_i^2})$$\n",
    "\n",
    "## PAC学习\n",
    "\n",
    "&emsp; 计算学习理论中最基本的是概率近似正确(PAC)。令$c$表示“概念”，这是从样本空间$X$到标记空间$Y$的映射，它决定实例$x$的真实标记$y$，若对任何样例$(x,y)$有$c(x)=y$成立，则称$c$为目标概念；所有我们希望学得的目标概念所构成的集合称为“概念类”，用符号$C$表示。\n",
    "\n",
    "&emsp; 给定一个学习算法，它所考虑的所有可能概念的集合称为假设空间，用符号$H$表示，由于学习算法事先不知道概念类的真实存在，因此$H$和$C$通常是不同的，学习算法会把自认为可能的目标概念集中起来构成$H$，对$h\\in H$，由于并不能确定它是否真是目标概念，因此称为假设。显然假设$h$也是从样本空间$X$到标记空间$Y$的映射。\n",
    "\n",
    "&emsp; 若目标概念$c\\in H$，则$H$中存在假设能将所有示例按与真是标记一致的方式完全分开，我们称该问题对该学习算法是可分的，亦称一致的；若$c\\notin H$，则$H$中不存在任何假设能将所有示例完全正确分开，称该问题对该学习算法是不可分的，亦称不一致的。\n",
    "\n",
    "&emsp; 给定训练集$D$，我们希望基于学习算法学得的模型所对应的假设$h$尽可能接近目标概念$c$。由于机器学习过程受到很多因素的制约，例如我们获得的训练集$D$往往仅包含有限数量的样例，因此通常会存在一些在$D$上“等效的”假设，学习算法对它们无法区别；同时，采样得到数据集合的过程有一定偶然性，即使对同样大小的不同训练集，学得结果可能也有所不同。因此，更希望以较大的把握学得比较好的模型，即以较大概率学得误差满足预设上限的模型，即概率近似正确。\n",
    "\n",
    "\n",
    "* PAC辨识(PAC Identify)：对$0<\\epsilon,\\delta<1$，所有$c\\in C$和分布$D$，若存在学习算法，其输出假设$h\\in H$满足$$P(E(h)\\leq\\epsilon)\\geq 1-\\delta$$，则称该学习算法能从假设空间$H$中PAC辨识概念类$C$，该学习算法能以较大概率(至少$1-\\delta$)学到的目标概念$c$的近似(误差最多为$\\epsilon$)\n",
    "\n",
    "\n",
    "* PAC可学习(PAC Learneble)：令$m$表示从分布$D$中独立同分布采样得到的样例数目，$0<\\epsilon,\\delta<1$，对所有分布$D$，若存在学习算法$L$和多项式函数$poly$，使得对于任何$m\\geq poly(1/\\epsilon,1/\\delta,size(x),size(c)),L$能从假设空间$H$中PAC辨识概念类$C$，则称概念类$C$对假设空间$H$而言是PAC可学习的\n",
    "\n",
    "\n",
    "* PAC学习算法(PAC Learning Algorithm)：若学习算法$L$使概念类$C$对假设空间$H$而言是PAC可学习的，且$L$的运行时间也是多项式函数$poly(1/\\epsilon,1/\\delta,size(x),size(c))$，则称概念类$C$是高效PAC可学习的，称$L$为概念类$C$的PAC学习算法\n",
    "\n",
    "\n",
    "* 样本复杂度(Sample Complexity)：满足PAC学习算法$L$所需的$m\\geq poly(1/\\epsilon,1/\\delta,size(x),size(c))$中最小的$m$，称为学习算法$L$的样本复杂度\n",
    "\n",
    "\n",
    "&emsp; PAC学习的一个关键因素即假设空间$H$的复杂度，$H$包含了学习算法$L$所有可能输出的假设，若在PAC学习中假设空间与概念类完全相同，即$H=C$，这称为“恰PAC可学习”；这意味着学习算法的能力与学习任务恰好匹配。然而在现实应用中概念类$C$通常一无所知，无法获得一个假设空间与概念类恰好相同的学习算法，更重要的是研究假设空间与概念类不同的情形，一般而言，$H$越大，其包含任意目标概念的可能性越大，但从中找到某个具体目标概念的难度也越大。$|H|$有限时，称其为“有限假设空间”，否则称为“无限假设空间”。\n",
    "\n",
    "## 有限假设空间\n",
    "\n",
    "### 可分情形\n",
    "\n",
    "&emsp; 可分情形意味着目标概念$c$属于假设空间$H$。对PAC学习来说，只要训练集$D$的规模能使学习算法以$1-\\delta$找到目标假设的$\\epsilon$近似即可。\n",
    "\n",
    "&emsp; 泛化误差大于$\\epsilon$但在训练集上仍表现完美的的假设出现的概率为$P((h(x_1)=y_1)\\bigcap\\cdots\\bigcap(h(x_m)=y_m))<(1-\\epsilon)^m$\n",
    "\n",
    "&emsp; 由于实现并不知道学习算法会输出$H$中的哪个假设，但仅需保证泛化误差大于$\\epsilon$但在训练集上仍表现完美的的所有假设出现概率之和不大于$\\delta$即可\n",
    "\n",
    "$$P(h\\in H: E(h)>\\epsilon\\bigcap\\hat{E}(h)=0)<|H|(1-\\epsilon)^m<|H|e^{-m\\epsilon}\\leq\\delta$$\n",
    "\n",
    "$$m\\geq \\frac{1}{\\epsilon}(ln|H|+ln\\frac{1}{\\delta})$$\n",
    "\n",
    "&emsp; 由此可知，有限假设空间$H$都是PAC可学习的，输出假设$h$的泛化误差随着样例数目的增多而收敛到0，收敛速率为$O(\\frac{1}{m})$\n",
    "\n",
    "### 不可分情形\n",
    "\n",
    "&emsp; 对较为困难的学习问题，目标概念$c$往往不存在于假设空间$H$中，假定对于任何$h\\in H,\\hat{E}(h)\\neq0$。\n",
    "\n",
    "* 若训练集$D$包含$m$个独立同分布采样而得的样例，$0\\leq\\epsilon\\leq1$，则对任意$h\\in H$，有$$P(\\hat{E}(h)-E(h)\\geq\\epsilon)\\leq exp(-2m\\epsilon^2)$$$$P(E(h)-\\hat{E}(h)\\geq\\epsilon)\\leq exp(-2m\\epsilon^2)$$$$P(|\\hat{E}(h)-E(h)|\\geq\\epsilon)\\leq 2exp(-2m\\epsilon^2)$$，可推得下式以至少$1-\\delta$概率成立：$$\\hat{E}(h)-\\sqrt{\\frac{ln(2/\\delta)}{2m}}\\leq E(h)\\leq\\hat{E}(h)+\\sqrt{\\frac{ln(2/\\delta)}{2m}}$$\n",
    "\n",
    "\n",
    "* 若$H$为有限假设空间，$0\\leq\\epsilon\\leq1$，则对任意$h\\in H$，有$$P(|\\hat{E}(h)-E(h)|\\leq\\sqrt{\\frac{ln|H|+ln(2/\\delta)}{2m}})\\geq1-\\delta$$\n",
    "\n",
    "* 不可知PAC学习：令$m$表示独立同分布采样得到的样例数目，$0\\leq\\epsilon,\\delta\\leq1$，对任意分布，若存在学习算$L$和多项式函数$poly$，使得对于任何$m\\geq poly(1/\\epsilon,1/\\delta,size(x),size(c)),L$能存假设空间$H$中输出如下假设$h$：$$P(E(h)-{min}_{h'\\in H}E(h')\\leq\\epsilon)\\geq1-\\delta$$则称假设空间$H$是不可知PAC学习的\n",
    "\n",
    "## VC维\n",
    "\n",
    "&emsp; 现实学习任务所面临的通常是无限假设空间，欲对此种情形的可学习性进行研究，需度量假设空间的复杂度，最常见的办法是考虑假设空间的“VC维”(Vapnik-Chervonenkis dimension)\n",
    "\n",
    "&emsp; 给定假设空间$H$和示例集$D=\\{x_1,x_2,\\cdots,x_m\\},H$中的每个假设$h$都能对$D$中示例赋予标记，标记结果可表示为$$h|_D=\\{h(x_1),h(x_2),\\cdots,h(x_m)\\}$$\n",
    "\n",
    "&emsp; 随着$m$的增大，$H$中所有假设对$D$中的示例所能赋予标记的可能结果数也会增大。\n",
    "\n",
    "* 对所有$m\\in N$，假设空间$H$的增长函数$\\prod_H(m)$为$$\\prod_H(m)=max_{\\{x_1,\\cdots,x_m\\}\\subseteq X}|\\{h(x_1),h(x_2),\\cdots,h(x_m)|h\\in H\\}|$$\n",
    "\n",
    "&emsp; 增长函数$\\prod_H(m)$表示假设空间$H$对$m$个示例所能赋予标记的最大可能结果数。显然，$H$对示例所能赋予标记的可能结果数越大，$H$的表示能力越强，对学习任务的适应能力也越强，因此，增长函数描述了假设空间$H$的表示能力，由此反映出假设空间的复杂度，可利用增长函数来估计经验误差欲泛化误差之间的关系：\n",
    "\n",
    "* 对假设空间$H,m\\in N,0\\leq\\epsilon\\leq1$和任意$h\\in H$有$$P(|E(h)-\\hat{E}(h)|>\\epsilon)\\leq4\\prod_H(2m)exp(-\\frac{m\\epsilon^2}{8})$$\n",
    "\n",
    "&emsp; 假设空间$H$中不同的假设对于示例赋予标记的结果可能相同，也可能不同；尽管$H$可能包含无穷多个假设，但其对示例赋予标记的可能结果数是有限的：对$m$个示例，最多有$2^m$个可能结果。对二分类问题来说，$H$中的假设对示例赋予标记的每种可能结果称为对示例集$D$的一种“对分”。若假设空间$H$能实现示例集上的所有对分，即$\\prod_H(m)=2^m$，则称示例集$D$能被假设空间$H$打散。\n",
    "\n",
    "* 假设空间$H$的VC维是能被$H$打散的最大示例集的大小，即$$VC(H)=max\\{m: \\prod_H(m)=2^m\\}$$\n",
    "\n",
    "* 若假设空间$H$的VC维是$d$，则对任意$m\\in N$有$$\\prod_H(m)\\leq\\sum_{i=0}^dC_m^i\\leq(\\frac{e\\cdot m}{d})^d$$\n",
    "\n",
    "* 任何VC维有限的假设空间$H$都是(不可知)PAC可学习的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
