{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest-Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN简介\n",
    "\n",
    "&emsp; 给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最近邻的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。\n",
    "\n",
    "## 距离度量\n",
    "\n",
    "&emsp; 常采用欧式距离\n",
    "\n",
    "<center>$L_2(x_i, x_j)=(\\sum_{l=1}^{n}{|x_i^{(l)}-x_j^{(l)}|^2})^\\frac{1}{2}$</center>\n",
    "\n",
    "## k值的选择\n",
    "\n",
    "&emsp; 如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测，近似误差(approximation)会减小，只有与输入实例较近的训练实例才会对预测起作用，缺点是估计误差(estimation error)会增大，预测结果对近邻的实例点非常敏感。\n",
    "\n",
    "&emsp; 如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测，优点是可以减少学习的估计误差，缺点是学习的近似误差会增大。\n",
    "\n",
    "## KNN的实现：kd树\n",
    "\n",
    "&emsp; kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的二叉树数据结构，kd树表示对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间划分，构成一系列k维超矩形区域，kd树每个节点对应于一个k维超矩形区域。\n",
    "\n",
    "&emsp; 构造平衡kd树：令$x_i=(x_i^{(1)}, x_i^{(2)}, \\cdots , x_i^{(k)})^T$：\n",
    "\n",
    "&emsp; 首先选择$x^{(1)}$为坐标轴，将所有实例的$x^{(1)}$的中位数为切分点，将根节点对应的超矩形区域切分成两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由根节点生成深度为1的左、右子节点：左子节点对应坐标$x^{(1)}$小于切分点的子区域，右子节点对应坐标$x^{(1)}$大于切分点的子区域。\n",
    "\n",
    "&emsp; 重复以下步骤：对深度为j的节点选择$x^{(l)}$为切分的坐标轴，$l=j(mod k)+1$，重复上述操作直到两个子区域没有实例存在时停止。\n",
    "\n",
    "&emsp; 搜索kd树：\n",
    "\n",
    "&emsp; 首先在kd树中找出包含目标点x的叶节点：从根节点出发，递归地向下访问kd树，直到子节点为叶节点为止，以此叶节点为“当前最近点”。\n",
    "\n",
    "&emsp; 递归地向上回退，在每个结点处，若该节点保存的实例点比当前最近点距离目标点更近，则该实例点变为当前最近点。同时检查另一子节点对应区域是否与以目标点为球心、以目标点与当前最近点间距离为半径的超球体相交，若相交，则在另一子节点对应区域内可能存在更近的点，进行搜索；若不相交，向上回退，回到根节点时，搜索结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor(object):\n",
    "  \"\"\" a kNN classifier with L2 distance \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def train(self, X, y):\n",
    "    self.X_train = X\n",
    "    self.y_train = y\n",
    "    \n",
    "  def predict(self, X, k=1):\n",
    "    num_test = X.shape[0]\n",
    "    num_train = self.X_train.shape[0]\n",
    "    dists = np.zeros((num_test, num_train)) \n",
    "    M = np.dot(X, self.X_train.T)\n",
    "    A1 = np.sum(np.square(X), axis=-1)\n",
    "    A2 = np.sum(np.square(self.X_train), axis=-1)\n",
    "    dists = np.sqrt(- 2 * M + A1.reshape(-1, 1) + A2.T)\n",
    "    return self.predict_labels(dists, k=k)\n",
    "\n",
    "  def predict_labels(self, dists, k=1):\n",
    "    num_test = dists.shape[0]\n",
    "    y_pred = np.zeros(num_test)\n",
    "    for i in range(num_test):\n",
    "      ind = np.argsort(dists[i], axis=-1)[:k]\n",
    "      labels = self.y_train[ind].flatten()\n",
    "      y_pred[i] = np.argmax(np.bincount(labels))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99 0.97\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = X[: 1697], y[: 1697]\n",
    "X_test, y_test = X[1697:], y[1697:]\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "model = KNearestNeighbor()\n",
    "model.train(X_train, y_train)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "print(np.sum(model.predict(X_test, 5) == y_test) / y_test.shape[0], neigh.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDNode():\n",
    "    def __init__(self, index, x, left, right):\n",
    "        self.index = index\n",
    "        self.x = x\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class KDTree():\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.root = self._create_node(X, 0)\n",
    "        return\n",
    "    \n",
    "    def _create_node(self, X, index):\n",
    "        N, D = X.shape\n",
    "        if not N:\n",
    "            return None\n",
    "        X = X[X[:, index].argsort()]\n",
    "        pos = N // 2\n",
    "        ind = (index + 1) % D\n",
    "        return KDNode(index, X[pos], self._create_node(X[:pos], ind), self._create_node(X[pos+1:], ind))\n",
    "        \n",
    "    def find_nearest(self, x):\n",
    "        if not self.root:\n",
    "            print(\"no training\")\n",
    "            return\n",
    "        return self._search(x, self.root)\n",
    "        \n",
    "    def _search(self, x, node):\n",
    "        if not node:\n",
    "            return None, np.inf\n",
    "        index = node.index\n",
    "        point = node.x\n",
    "        value = point[index]\n",
    "        \n",
    "        if x[node.index] <= value:\n",
    "            nearest, near_distance = self._search(x, node.left)\n",
    "            other = node.right\n",
    "        else:\n",
    "            nearest, near_distance = self._search(x, node.left)\n",
    "            other = node.left\n",
    "        \n",
    "        if near_distance < np.abs(x[node.index] - value):\n",
    "            return nearest, near_distance\n",
    "        else:\n",
    "            distance = np.linalg.norm(x - point)\n",
    "            if distance < near_distance:\n",
    "                nearest = point\n",
    "                near_distance = distance\n",
    "            other_point, other_distance = self._search(x, other)\n",
    "            if other_distance < near_distance:\n",
    "                return other_point, other_distance\n",
    "            return nearest, near_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  24.21884052249912 s\n",
      "(array([-1.12635325, -0.00344532,  0.13906767]), 1.4812936983871998)\n"
     ]
    }
   ],
   "source": [
    "from time import clock\n",
    "\n",
    "N = 400000\n",
    "t0 = clock()\n",
    "kd = KDTree()\n",
    "kd.fit(np.random.randn(N, 3))\n",
    "result = kd.find_nearest([0.1,0.5,0.8])\n",
    "t1 = clock()\n",
    "print (\"time: \",t1-t0, \"s\")\n",
    "print (result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
