{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类\n",
    "\n",
    "## 性能度量\n",
    "\n",
    "&emsp; 聚类性能度量大致有两类。一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考察聚类结果而不利用任何参考模型，称为内部指标。\n",
    "\n",
    "&emsp; 对数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，假定通过聚类给出的簇划分为$C=\\{C_1,C_2,\\cdots,C_k\\}$，相应地，令$\\lambda$与$\\lambda^{*}$分别表示$C$与$C_{*}$对应的簇标记向量，将样本两两配对，定义\n",
    "\n",
    "$$a=|SS|,SS=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{*}=\\lambda_j^{*},i<j\\}$$\n",
    "\n",
    "$$b=|SD|,SD=\\{(x_i,x_j)|\\lambda_i=\\lambda_j,\\lambda_i^{*}\\neq\\lambda_j^{*},i<j\\}$$\n",
    "\n",
    "$$c=|DS|,DS=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{*}=\\lambda_j^{*},i<j\\}$$\n",
    "\n",
    "$$d=|DD|,DD=\\{(x_i,x_j)|\\lambda_i\\neq\\lambda_j,\\lambda_i^{*}\\neq\\lambda_j^{*},i<j\\}$$\n",
    "\n",
    "* Jaccard系数\n",
    "\n",
    "$$JC=\\frac{a}{a+b+c}$$\n",
    "\n",
    "* FM指数\n",
    "\n",
    "$$FMI=\\sqrt{\\frac{a}{a+b}\\cdot\\frac{a}{a+c}}$$\n",
    "\n",
    "* Rand指数\n",
    "\n",
    "$$RI=\\frac{2(a+d)}{m(m-1)}$$\n",
    "\n",
    "&emsp; 显然，上述性能度量的结果值均在$[0,1]$之间，值越大越好\n",
    "\n",
    "&emsp; 对数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，假定通过聚类给出的簇划分为$C=\\{C_1,C_2,\\cdots,C_k\\}$，定义\n",
    "\n",
    "$$avg(C)=\\frac{2}{|C|(|C|-1)}\\sum_{1\\leq i<j\\leq |C|}dist(x_i,x_j)$$\n",
    "\n",
    "$$diam(C)=max_{1\\leq i<j\\leq |C|}dist(x_i,x_j)$$\n",
    "\n",
    "$$d_min(C_i,C_j)=min_{x_i\\in C_i,x_j\\in C_j}dist(x_i,x_j)$$\n",
    "\n",
    "$$d_{cen}(C_i,C_j)=dist(\\mu_i,\\mu_j)$$\n",
    "\n",
    "* DB指数\n",
    "\n",
    "$$DBI=\\frac{1}{k}\\sum_{i=1}^kmax_{j\\neq i}(\\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)})$$\n",
    "\n",
    "* Dunn指数\n",
    "\n",
    "$$DI=min_{1\\leq i \\leq k}\\{min_{j\\neq i}(\\frac{d_{min}(C_i,C_j)}{max_{1\\leq l \\leq k}diam(C_l)})\\}$$\n",
    "\n",
    "&emsp; 显然，$DBI$的值越小越好，$DI$的值越大越好\n",
    "\n",
    "## 原型聚类\n",
    "\n",
    "&emsp; 原型聚类假设聚类结构能通过样本空间中一组代表性的点刻画。\n",
    "\n",
    "### k均值算法\n",
    "\n",
    "&emsp; k均值算法目标为最小化均方误差\n",
    "\n",
    "$$E=\\sum_{i=1}^k\\sum_{x\\in C_i}||x-\\mu_i||_2^2$$\n",
    "\n",
    "* 首先从样本集中随机选择$k$个样本作为初始均值向量$\\{\\mu_1,\\mu_2,\\cdots,\\mu_k\\}$\n",
    "\n",
    "* 计算每个样本与各均值向量的距离，根据距离最近的均值向量确定样本的簇标记，计算新的均值向量$\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$，直到所有均值向量不再更新\n",
    "\n",
    "### 学习向量量化\n",
    "\n",
    "* 初始化一组原型向量$\\{p_1,p_2,\\cdots,p_q\\}$\n",
    "\n",
    "* 随机选择一个样本，计算该样本与各原型向量的距离，根据距离最近的原型向量$p$，若该样本的类标记与原型向量的类标记相同，则$p=p+\\eta\\cdot(x-p)$，否则$p=p-\\eta\\cdot(x-p)$，$\\eta$为学习率，重复直到满足停机条件\n",
    "\n",
    "### 高斯混合聚类\n",
    "\n",
    "&emsp; 定义高斯混合分布\n",
    "\n",
    "$$p(x)=\\sum_{i=1}^k\\alpha_i\\cdot p(x|\\mu_i,\\Sigma_i)\\ \\ \\sum_{i=1}^k\\alpha_i=1$$\n",
    "\n",
    "&emsp; 若训练集$D={x_1,x_2,\\cdots,x_m}$由上述过程生成，令随机变量$z_j\\in \\{1,2,\\cdots,k\\}$表示生成样本的高斯混合成分，其取值未知。显然，$z_j$的先验概率$P(z_j=i)$对应于$\\alpha_i$。根据贝叶斯定理，$z_j$的后验分布对应于\n",
    "\n",
    "$$\\gamma_{ji}=p(z_j=i|x_j)=\\frac{\\alpha_i\\cdot p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^k\\alpha_l\\cdot p(x_j|\\mu_l,\\Sigma_l)}$$\n",
    "\n",
    "&emsp; 当高斯混合分布已知时，高斯混合聚类把样本$D$划分为k个簇，每个样本的簇标记如下确定\n",
    "\n",
    "$$\\lambda_j=argmax_{i\\in \\{1,2,\\cdots,k\\}}\\gamma_{ji}$$\n",
    "\n",
    "* 计算$x_j$由各混合成分生成的后验概率$\\gamma_{ji}$\n",
    "\n",
    "* 计算新的均值向量$\\mu_i=\\frac{\\sum_{j=1}^m\\gamma_{ji}x_j}{\\sum_{j=1}^m\\gamma_{ji}}$\n",
    "\n",
    "* 计算新的协方差矩阵$\\Sigma_i=\\frac{\\sum_{j=1}^m\\gamma_{ji}(x_j-\\mu_i)(x_j-\\mu_i)^T}{\\sum_{j=1}^m\\gamma_{ji}}$\n",
    "\n",
    "* 计算新的混合系数$\\alpha_i=\\frac{\\sum_{j=1}^m\\gamma_{ji}}{m}$\n",
    "\n",
    "* 重复上述步骤直至满足停机条件\n",
    "\n",
    "## 密度聚类\n",
    "\n",
    "&emsp; 密度聚类假设聚类结构能通过样本分布的紧密程度确定，通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终结果。\n",
    "\n",
    "&emsp; DBSCAN是一种著名的聚类算法，它基于一组“邻域”参数来刻画样本分布的紧密程度，给定数据集$D=\\{x_1,x_2,\\cdots,x_m\\}$，定义如下几个概念：\n",
    "\n",
    "* $\\epsilon$-邻域：对$x_j\\in D$，其$\\epsilon$-邻域包含样本集$D$中与$x_j$的距离不大于$\\epsilon$的样本，即$N_{\\epsilon}(x_j)=\\{x_i\\in D|dist(x_i,x_j)\\leq \\epsilon\\}$\n",
    "\n",
    "* 核心对象：若$x_j$的$\\epsilon$-邻域至少包含MinPts个样本，即$|N_{\\epsilon}(x_j)|\\geq MinPts$，则$x_j$是一个核心对象\n",
    "\n",
    "* 密度直达：若$x_j$位于$x_i$的$\\epsilon$-邻域中，且$x_i$是核心对象，则称$x_j$由$x_i$核心直达\n",
    "\n",
    "* 密度可达：对$x_i$与$x_j$，若存在样本序列$p_1,p_2,\\cdots,p_n$，其中$p_1=x_i,p_n=x_j$且$p_{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达\n",
    "\n",
    "* 密度相连：对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连\n",
    "\n",
    "&emsp; 基于这些概念，DBSCAN将“簇”定义为：由密度可达关系导出说我最大的密度相连样本集合。基于此，得出DBSCAN算法\n",
    "\n",
    "* 找出样本集中所有的核心对象，组成未访问核心对象集合\n",
    "\n",
    "* 从未访问核心对象集合随机选择一个核心对象，找到未访问样本集合中所有密度可达点，形成新的簇，将新的簇中包含的点从未访问样本集合与未访问核心对象集合中删除"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
