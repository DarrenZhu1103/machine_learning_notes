{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性可分支持向量机与硬间隔最大化\n",
    "\n",
    "### 线性可分支持向量机\n",
    "\n",
    "&emsp; 给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面以及相应的分类决策函数为\n",
    "\n",
    "$$w \\cdot x + b = 0$$\n",
    "\n",
    "$$f(x) = sign(w \\cdot x + b)$$\n",
    "\n",
    "### 函数间隔与几何间隔\n",
    "\n",
    "&emsp; 定义超平面$(w, b)$关于样本点$(x_i, y_i)$的函数间隔与几何间隔为\n",
    "\n",
    "$$\\hat{\\gamma}_i=y_i(w \\cdot x + b)$$\n",
    "\n",
    "$$\\gamma_i=y_i(\\frac{w}{||w||} \\cdot x + \\frac{b}{||w||})$$\n",
    "\n",
    "&emsp; 定义超平面$(w, b)$关于给定训练集$T$的函数间隔与几何间隔分别为超平面关于训练集中所有样本点的函数间隔与几何间隔的最小值。\n",
    "\n",
    "### 间隔最大化\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "max_{w, b} \\ &\\frac{\\hat{\\gamma}}{||w||}\\\\\n",
    "s.t. \\ &y_i(w \\cdot x + b) >= \\hat{\\gamma}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "&emsp; 改写为一个凸二次规划问题\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "min_{w, b} \\ &\\frac{1}{2}||w||^2\\\\\n",
    "s.t. \\ &y_i(w \\cdot x + b) - 1 >= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "&emsp; 在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量，分离超平面两侧的两个超平面即间隔边界。\n",
    "\n",
    "### 对偶算法\n",
    "\n",
    "&emsp; 构建拉格朗日函数\n",
    "\n",
    "$$L(w, b, \\alpha) = \\frac{1}{2}||w||^2+\\sum^m_{i=1}\\alpha_i(-y_i(w^Tx_i+b)+1)$$\n",
    "\n",
    "&emsp; 根据拉格朗日对偶性，将原始问题化为对偶问题\n",
    "\n",
    "$$\\min\\ f(x)=\\min_{w, b} \\max_{\\alpha}\\ L(w, b, \\alpha)\\geq \\max_{\\alpha} \\min_{w, b}\\ L(w, b, \\alpha)$$\n",
    "\n",
    "&emsp; 先求解最小化问题，得\n",
    "\n",
    "$$w = \\sum_{i=1}^N\\alpha_iy_ix_i$$\n",
    "\n",
    "$$\\sum_{i=1}^N\\alpha_iy_i=0$$\n",
    "\n",
    "&emsp; 最终得到对偶问题\n",
    "\n",
    "$$\\max\\ \\sum^m_{i=1}\\alpha_i-\\frac{1}{2}\\sum^m_{i,j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j)=\\min \\frac{1}{2}\\sum^m_{i,j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j)-\\sum^m_{i=1}\\alpha_i$$\n",
    "\n",
    "$$s.t.\\ \\sum^m_{i=1}\\alpha_iy_i=0,$$\n",
    "\n",
    "$$ \\alpha_i \\geq 0,i=1,2,...,m$$\n",
    "\n",
    "&emsp; 解得\n",
    "\n",
    "$$w = \\sum_{i=1}^N\\alpha_iy_ix_i$$\n",
    "\n",
    "$$b = y_j-w \\cdot x_j$$\n",
    "\n",
    "&emsp; 这里取任意$y_j$满足$\\alpha_i > 0$\n",
    "\n",
    "## 线性支持向量机与软间隔最大化\n",
    "\n",
    "&emsp; 对于线性不可分训练数据，学习问题变为解如下凸二次规划\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "min_{w, b, \\xi} \\ &\\frac{1}{2}||w||^2+C\\sum_{i=1}^{N}\\xi_i\\\\\n",
    "s.t. \\ &y_i(w \\cdot x + b) - 1 + \\xi_i >= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "&emsp; 原问题的对偶问题是\n",
    "\n",
    "$$\\max\\ \\sum^N_{i=1}\\alpha_i-\\frac{1}{2}\\sum^N_{i,j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j)=\\min \\frac{1}{2}\\sum^N_{i,j=1}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j)-\\sum^N_{i=1}\\alpha_i$$\n",
    "\n",
    "$$s.t.\\ \\sum^m_{i=1}\\alpha_iy_i=0,$$\n",
    "\n",
    "$$ 0 \\leq \\alpha_i \\leq C,i=1,2,...,N$$\n",
    "\n",
    "&emsp; 解得\n",
    "\n",
    "$$w = \\sum_{i=1}^N\\alpha_iy_ix_i$$\n",
    "\n",
    "$$b = y_j-w \\cdot x_j$$\n",
    "\n",
    "&emsp; 这里取任意$y_j$满足0 < $\\alpha_i < C$\n",
    "\n",
    "## 非线性支持向量机与核函数\n",
    "\n",
    "&emsp; 对于线性不可分的训练数据集，可通过一个非线性变换将输入空间对应于一个高维（甚至无穷维）的特征空间，使得在输入空间的超曲面模型对应于在特征空间中的超平面模型，这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。\n",
    "\n",
    "### 核函数\n",
    "\n",
    "&emsp; 设$X$是输入空间，$H$是特征空间，若存在一个映射：$\\phi(x):X\\rightarrow H$，使得对所有$x, z \\in X$，函数$K(x, z)$满足条件：\n",
    "\n",
    "$$K(x, z)=\\phi(x) \\cdot \\phi(z)$$\n",
    "\n",
    "&emsp; 则称$K(x, z)$为核函数，$\\phi(x)$为映射函数。核技巧在学习和预测中置顶义核函数$K(x, z)$，而不显示定义映射函数，通常直接计算$K(x, z)$比较容易，而通过$\\phi(x) \\cdot \\phi(z)$内积计算并不容易。\n",
    "\n",
    "&emsp; 在线性支持向量机的对偶问题中，目标函数与决策函数都涉及输入实例与实例之间的内积，这都可以用核函数来代替。学习是隐式地在特征空间进行的，不需要显式定义特征空间与映射函数。\n",
    "\n",
    "$$\\min \\frac{1}{2}\\sum^N_{i,j=1}\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)-\\sum^N_{i=1}\\alpha_i$$\n",
    "\n",
    "$$f(x) = sign(\\sum_{i=1}^N\\alpha_iy_iK(x_i, x) + b)$$\n",
    "\n",
    "### 常用核函数\n",
    "\n",
    "#### 多项式核函数\n",
    "\n",
    "$$K(x, z) = (x \\cdot z + 1)^p$$\n",
    "\n",
    "#### 高斯核函数\n",
    "\n",
    "$$K(x, z) = exp(-\\frac{||x - z||^2}{2\\sigma^2})$$\n",
    "\n",
    "## Sequential Minimal Optimizaton\n",
    "\n",
    "&emsp; SMO算法为一种启发式算法，其基本思路为：若所有变量的解都满足此最优化问题的KKT条件，那么得到最优解。否则，选择两个变量，固定其他变量，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
