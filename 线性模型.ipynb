{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型\n",
    "\n",
    "## 线性回归\n",
    "\n",
    "&emsp; 线性回归试图学得\n",
    "\n",
    "$$f(x)=w^Tx$$\n",
    "\n",
    "&emsp; 学习目标为让均方误差最小化\n",
    "\n",
    "$$w^*=argmin_w\\sum_{i=1}^m(f(x_i)-y_i)^2$$\n",
    "\n",
    "&emsp; 基于均方误差最小化来进行模型求解的方法称为最小二乘法，在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之和最小。最小二乘法的多元线性回归最优解为\n",
    "\n",
    "$$\\hat{w}^*=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "&emsp; 更一般地，考虑单调可微函数$g$，令\n",
    "\n",
    "$$y=g^{-1}(w^Tx)$$\n",
    "\n",
    "这样得到的模型称为广义线性模型，函数$g$称为联系函数。\n",
    "\n",
    "## 线性判别分析\n",
    "\n",
    "&emsp; 线性判别分析(Linear Discriminant Analysis)，简称LDA，是一种经典的线性学习方法。LDA的思想十分朴素：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，同样将其投影到该直线上，根据投影点的位置来确定新样本。\n",
    "\n",
    "&emsp; 根据给定数据集$D=\\{(x_i,y_i)\\}_{i=1}^m$，$y_i\\in {0,1}$，令$X_i,\\mu_i,\\Sigma_i$\n",
    "\n",
    "## 多分类学习\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
