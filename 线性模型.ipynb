{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型\n",
    "\n",
    "## 线性回归\n",
    "\n",
    "&emsp; 线性回归试图学得\n",
    "\n",
    "$$f(x)=w^Tx$$\n",
    "\n",
    "&emsp; 学习目标为让均方误差最小化\n",
    "\n",
    "$$w^*=argmin_w\\sum_{i=1}^m(f(x_i)-y_i)^2$$\n",
    "\n",
    "&emsp; 基于均方误差最小化来进行模型求解的方法称为最小二乘法，在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之和最小。最小二乘法的多元线性回归最优解为\n",
    "\n",
    "$$\\hat{w}^*=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "&emsp; 更一般地，考虑单调可微函数$g$，令\n",
    "\n",
    "$$y=g^{-1}(w^Tx)$$\n",
    "\n",
    "这样得到的模型称为广义线性模型，函数$g$称为联系函数。\n",
    "\n",
    "## 线性判别分析\n",
    "\n",
    "&emsp; 线性判别分析(Linear Discriminant Analysis)，简称LDA，是一种经典的线性学习方法。LDA的思想十分朴素：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，同样将其投影到该直线上，根据投影点的位置来确定新样本。\n",
    "\n",
    "&emsp; 根据给定数据集$D=\\{(x_i,y_i)\\}_{i=1}^m$，$y_i\\in {0,1}$，令$X_i,\\mu_i,\\Sigma_i$分别表示第$i\\in\\{0,1\\}$类的集合、均值向量、协方差矩阵，若将数据投影到直线$w$上，则两类样本的中心在直线上的投影分别为$w^T\\mu_0,\\ w^T\\mu_1$，协方差分别为$w^T\\Sigma_0w,\\ w^T\\Sigma_1w$\n",
    "\n",
    "&emsp; 欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$w^T\\Sigma_0w+w^T\\Sigma_1w$尽可能小；而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$||w^T\\mu_0-w^T\\mu_1||_2^2$尽可能大，同时考虑两者，则可得到最大化目标\n",
    "\n",
    "$$J=\\frac{||w^T\\mu_0-w^T\\mu_1||_2^2}{w^T\\Sigma_0w+w^T\\Sigma_1w}=\\frac{w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^Tw}{w^T(\\Sigma_0+\\Sigma_1)w}=\\frac{w^TS_bw}{w^TS_ww}$$\n",
    "\n",
    "&emsp; 求$S_w^{-1}S_b$的特征向量即可"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
